# Руководство по локальной установке и использованию Gemma

Это руководство поможет вам установить и запустить модель Google Gemma локально на вашем компьютере и начать её использовать для различных задач.

---

## Что такое Gemma и зачем запускать локально?

**Gemma** — это семейство лёгковесных открытых моделей от Google, созданных на основе тех же исследований, что и модели Gemini. Они разработаны для обеспечения доступа к передовым технологиям ИИ для разработчиков и исследователей.

**Преимущества локального развертывания:**

- **Конфиденциальность:** ваши данные не покидают локальную среду.
- **Экономия:** отсутствие расходов на облачные API.
- **Офлайн-доступ:** работа без подключения к интернету.
- **Полный контроль:** возможность тонкой настройки и экспериментов.

---

## Требования

1. **Аппаратные:**
   - **CPU:** современный многоядерный процессор.
   - **RAM:** минимум 8 ГБ (рекомендуется 16 ГБ+ для моделей 7B).
   - **GPU (рекомендуется):**
     - NVIDIA (CUDA), AMD (ROCm) или Apple Silicon (Metal).
     - VRAM: 6–8 ГБ для Gemma 2B, 10–12 ГБ для Gemma 7B (зависит от квантования).
   - Если GPU нет — модель работает на CPU, но медленнее.
   - **Место на диске:** несколько–десят гигабайт (в зависимости от модели и квантования).

2. **Программные:**
   - **Python 3.8+**
   - **pip** (менеджер пакетов)
   - **llama.cpp** + **llama-cpp-python** (Python‑биндинги для эффективного запуска GGUF-моделей)
   - **Драйверы GPU:** CUDA, ROCm или Metal (по необходимости)

---

## Шаг 1. Загрузка модели Gemma (формат GGUF)

1. Перейдите на Hugging Face: https://huggingface.co
2. В поиске введите `gemma gguf thebloke`.
3. Выберите нужный репозиторий (например, `gemma-2b-it-GGUF` или `gemma-7b-it-GGUF`).
4. Во вкладке **Files and versions** скачайте файл с расширением `.gguf`.
   - Для экономии памяти выбирайте квантованные версии (Q4_K_M, Q5_K_M и т.п.).
5. Запомните путь к скачанному файлу `.gguf`.

---

## Шаг 2. Установка llama-cpp-python

> **llama-cpp-python** — Python‑интерфейс к библиотеке llama.cpp, оптимизированной для локального запуска LLM.

```bash
# Установка (только CPU)
pip install llama-cpp-python
```

### С поддержкой GPU

**NVIDIA (CUDA):**
```bash
pip uninstall llama-cpp-python  # при необходимости
pip install llama-cpp-python[cuda]
```

**Apple Silicon (Metal):**
```bash
pip uninstall llama-cpp-python
pip install llama-cpp-python[metal]
```

**AMD (ROCm):**
```bash
pip uninstall llama-cpp-python
pip install llama-cpp-python[rocm]
```

При необходимости ознакомьтесь с документацией llama-cpp-python на GitHub для специфичных флагов сборки.

---

## Шаг 3. Пример скрипта на Python

Создайте файл `run_gemma.py` и вставьте следующий код:

```python
from llama_cpp import Llama
import os

# Путь к файлу модели GGUF
model_path = "ПУТЬ_К_ВАШЕЙ_МОДЕЛИ_GGUF"

if not os.path.exists(model_path):
    print(f"Ошибка: модель не найдена по пути: {model_path}")
    exit()

print("Загрузка модели Gemma...")
llm = Llama(
    model_path=model_path,
    n_gpu_layers=-1,    # выгрузить все слои на GPU
    n_ctx=4096,
    verbose=True
)
print("Модель загружена.")

# Тест генерации текста
prompt = '''
<start_of_turn>user
Напиши короткий рассказ о коте, который умеет летать.
<end_of_turn>
<start_of_turn>model
'''

output = llm(
    prompt,
    max_tokens=256,
    stop=["<end_of_turn>"],
    temperature=0.7,
    echo=False
)

text = output["choices"][0]["text"].strip()
print("\nСгенерированный текст:")
print(text)
```

> **Важно:** замените `model_path` на путь к вашему `.gguf` файлу.

---

## Шаг 4. Запуск скрипта

```bash
python run_gemma.py
```

- При первом запуске загрузка модели может занять время.
- Вы увидите прогресс загрузки и результат генерации текста.

---

## Дальнейшие шаги

- **Экспериментируйте с промптами:** меняйте запросы и инструкционные форматы.
- **Настройка генерации:** регулировка `max_tokens`, `temperature`, `stop`.
- **Интерактивный режим:** раскомментируйте цикл в примере и общайтесь в реальном времени.
- **Интеграция в приложения:** используйте Flask, FastAPI, Streamlit и пр.
- **Изучение llama.cpp и Hugging Face Transformers** для более глубокого контроля.

---

*Успехов в работе с моделью Gemma!*  
```

